{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCI-alxogm/ml2023-Isaias-Hernandez/blob/main/Ago25/gradiente_descendiente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa8394f",
      "metadata": {
        "id": "efa8394f"
      },
      "source": [
        "# Gradiente Descendiente"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da11ed60",
      "metadata": {
        "id": "da11ed60"
      },
      "source": [
        "El enfoque más simple para utilizar información de gradiente es elegir la actualización de pesos en la ecuación $ w^{\\tau +1} = w^{\\tau} -  \\Delta w^{\\tau}$ de manera que comprenda un pequeño paso en la dirección del gradiente negativo, de modo que\n",
        "\n",
        "$$\n",
        "    \\boldsymbol{w}^{\\tau +1} = \\boldsymbol{w}^{\\tau} - \\eta \\nabla E(\\boldsymbol{w}^{\\tau})\n",
        "$$\n",
        "\n",
        "donde el parámetro $η > 0$ se conoce como tasa de aprendizaje (_learning rate_). Después de cada una de estas actualizaciones, el gradiente se reevalúa para el nuevo vector de pesos y el proceso se repite. Ten en cuenta que la función de error está definida con respecto a un conjunto de entrenamiento, por lo que cada paso requiere que todo el conjunto de entrenamiento se procese para evaluar $\\nabla E$. Las técnicas que utilizan todo el conjunto de datos de una vez se llaman _batch methods_. En cada paso, el vector de pesos se mueve en la dirección de la tasa más grande de disminución de la función de error, y por lo tanto, este enfoque se conoce como _gradient descent_ (descenso de gradiente) o _steepest descent_ (descenso más empinado). Aunque este enfoque podría parecer razonable de manera intuitiva, de hecho, resulta ser un algoritmo deficiente, por razones que se discuten en Bishop y Nabney (2008).\n",
        "\n",
        "\n",
        "Sin embargo, existe una versión on-line del descenso de gradiente que ha demostrado ser útil en la práctica para entrenar redes neuronales en conjuntos de datos grandes (Le Cun et al., 1989).\n",
        "-Pattern Recognition and Machine Learning\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f28d2dcb",
      "metadata": {
        "id": "f28d2dcb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "789ae5fc",
      "metadata": {
        "id": "789ae5fc"
      },
      "outputs": [],
      "source": [
        "def Polinomio(grado):\n",
        "    def Modelo(x, w):\n",
        "        y = 0\n",
        "        for i in range(grado + 1):\n",
        "            y += w[i] * (x ** i)\n",
        "        return y\n",
        "    return Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "11fdb540",
      "metadata": {
        "id": "11fdb540"
      },
      "outputs": [],
      "source": [
        "def GradienteDescendiente(x, y, modelo, valores_iniciales, tasa_aprendizaje=1, iteraciones=1000, tolerancia=0.001):\n",
        "    w = valores_iniciales\n",
        "    t = y\n",
        "    Costo = lambda x, w: 1/(2) * np.sum((modelo(x, w) - t)**2)\n",
        "    costo = Costo(x, w)\n",
        "    iteracion = 0\n",
        "    while iteracion < iteraciones:\n",
        "        #wn = np.empty_like(w)\n",
        "        for i in range(np.shape(w)[0]):\n",
        "            derivada_w = 1/1 * np.sum((modelo(x, w) - t)) * x**i\n",
        "            w[i] = w[i] - tasa_aprendizaje * derivada_w\n",
        "\n",
        "        costo = Costo(x, w)\n",
        "        if costo < tolerancia:\n",
        "          break\n",
        "        iteracion += 1\n",
        "    return w, costo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5291dab2",
      "metadata": {
        "id": "5291dab2"
      },
      "outputs": [],
      "source": [
        "datos = np.loadtxt(\"/content/drive/MyDrive/ML2023/olympic100m_men.txt\", delimiter=',')\n",
        "datos_x = datos[0]\n",
        "datos_y = datos[1]\n",
        "grado = 3\n",
        "w0 = np.random.uniform(-50, 50,grado+1)\n",
        "f = Polinomio(grado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "83ae157c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "83ae157c",
        "outputId": "0e788aa4-eade-48a2-8789-08ec3b0a0f2d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-87963db041a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradienteDescendiente\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatos_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-61623e25405b>\u001b[0m in \u001b[0;36mGradienteDescendiente\u001b[0;34m(x, y, modelo, valores_iniciales, tasa_aprendizaje, iteraciones, tolerancia)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mderivada_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtasa_aprendizaje\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mderivada_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcosto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCosto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ],
      "source": [
        "w, costo = GradienteDescendiente(datos_x, datos_y, f, w0, grado)\n",
        "x = np.linspace(np.max(datos_x), np.min(datos_x), 1000)\n",
        "plt.plot(x, f(x,w))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2862c4ca",
      "metadata": {
        "id": "2862c4ca"
      },
      "source": [
        "Notas\n",
        "* Valor inciales de **w**\n",
        "* Función costo\n",
        "$$L = \\frac{1}{2N} \\sum_{i}\\left(f(x,w)-t_{i}\\right)²$$\n",
        "* Derivada\n",
        "$$ \\frac{\\partial L}{\\partial w_{i}} = \\frac{1}{N} \\sum \\left( f(x,w) - t_{i} \\right) \\cdot \\frac{\\partial f}{\\partial w_i} $$\n",
        "* Actualización de pesos:\n",
        "$$ w_{i_nuevo} = w_{i} - \\alpha \\frac{\\partial L}{\\partial w_i}$$\n",
        "* Iteraciones\n",
        "* Determinar tolerancia"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}